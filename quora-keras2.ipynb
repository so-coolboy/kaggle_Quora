{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#设置随机种子保证可重复性\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入数据，预处理，包括改为小写和替换数字和url链接\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train[\"question_text\"] = train[\"question_text\"].str.lower()\n",
    "test[\"question_text\"] = test[\"question_text\"].str.lower()\n",
    "\n",
    "def clean_tag(text):\n",
    "    if '[math]' in text:\n",
    "        text = re.sub('\\[math\\].*?math\\]', '[formula]', text)\n",
    "    if 'http' in text or 'www' in text:\n",
    "        text = re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', '[url]', text)\n",
    "    return text\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_tag(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts=[',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n",
    "        '•', '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', \n",
    "        '█', '…', '“', '★', '”', '–', '●', '►', '−', '¢', '¬', '░', '¡', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', \n",
    "        '—', '‹', '─', '▒', '：', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '¸', '⋅', '‘', '∞', \n",
    "        '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '・', '╦', '╣', '╔', '╗', '▬', '❤', '≤', '‡', '√', '◄', '━', \n",
    "        '⇒', '▶', '≥', '╝', '♡', '◊', '。', '✈', '≡', '☺', '✔', '↵', '≈', '✓', '♣', '☎', '℃', '◦', '└', '‟', '～', '！', '○', \n",
    "        '◆', '№', '♠', '▌', '✿', '▸', '⁄', '□', '❖', '✦', '．', '÷', '｜', '┃', '／', '￥', '╠', '↩', '✭', '▐', '☼', '☻', '┐', \n",
    "        '├', '«', '∼', '┌', '℉', '☮', '฿', '≦', '♬', '✧', '〉', '－', '⌂', '✖', '･', '◕', '※', '‖', '◀', '‰', '\\x97', '↺', \n",
    "        '∆', '┘', '┬', '╬', '،', '⌘', '⊂', '＞', '〈', '⎙', '？', '☠', '⇐', '▫', '∗', '∈', '≠', '♀', '♔', '˚', '℗', '┗', '＊', \n",
    "        '┼', '❀', '＆', '∩', '♂', '‿', '∑', '‣', '➜', '┛', '⇓', '☯', '⊖', '☀', '┳', '；', '∇', '⇑', '✰', '◇', '♯', '☞', '´', \n",
    "        '↔', '┏', '｡', '◘', '∂', '✌', '♭', '┣', '┴', '┓', '✨', '\\xa0', '˜', '❥', '┫', '℠', '✒', '［', '∫', '\\x93', '≧', '］', \n",
    "        '\\x94', '∀', '♛', '\\x96', '∨', '◎', '↻', '⇩', '＜', '≫', '✩', '✪', '♕', '؟', '₤', '☛', '╮', '␊', '＋', '┈', '％', \n",
    "        '╋', '▽', '⇨', '┻', '⊗', '￡', '।', '▂', '✯', '▇', '＿', '➤', '✞', '＝', '▷', '△', '◙', '▅', '✝', '∧', '␉', '☭', \n",
    "        '┊', '╯', '☾', '➔', '∴', '\\x92', '▃', '↳', '＾', '׳', '➢', '╭', '➡', '＠', '⊙', '☢', '˝', '∏', '„', '∥', '❝', '☐', \n",
    "        '▆', '╱', '⋙', '๏', '☁', '⇔', '▔', '\\x91', '➚', '◡', '╰', '\\x85', '♢', '˙', '۞', '✘', '✮', '☑', '⋆', 'ⓘ', '❒', \n",
    "        '☣', '✉', '⌊', '➠', '∣', '❑', '◢', 'ⓒ', '\\x80', '〒', '∕', '▮', '⦿', '✫', '✚', '⋯', '♩', '☂', '❞', '‗', '܂', '☜', \n",
    "        '‾', '✜', '╲', '∘', '⟩', '＼', '⟨', '·', '✗', '♚', '∅', 'ⓔ', '◣', '͡', '‛', '❦', '◠', '✄', '❄', '∃', '␣', '≪', '｢', \n",
    "        '≅', '◯', '☽', '∎', '｣', '❧', '̅', 'ⓐ', '↘', '⚓', '▣', '˘', '∪', '⇢', '✍', '⊥', '＃', '⎯', '↠', '۩', '☰', '◥', \n",
    "        '⊆', '✽', '⚡', '↪', '❁', '☹', '◼', '☃', '◤', '❏', 'ⓢ', '⊱', '➝', '̣', '✡', '∠', '｀', '▴', '┤', '∝', '♏', 'ⓐ', \n",
    "        '✎', ';', '␤', '＇', '❣', '✂', '✤', 'ⓞ', '☪', '✴', '⌒', '˛', '♒', '＄', '✶', '▻', 'ⓔ', '◌', '◈', '❚', '❂', '￦', \n",
    "        '◉', '╜', '̃', '✱', '╖', '❉', 'ⓡ', '↗', 'ⓣ', '♻', '➽', '׀', '✲', '✬', '☉', '▉', '≒', '☥', '⌐', '♨', '✕', 'ⓝ', \n",
    "        '⊰', '❘', '＂', '⇧', '̵', '➪', '▁', '▏', '⊃', 'ⓛ', '‚', '♰', '́', '✏', '⏑', '̶', 'ⓢ', '⩾', '￠', '❍', '≃', '⋰', '♋', \n",
    "        '､', '̂', '❋', '✳', 'ⓤ', '╤', '▕', '⌣', '✸', '℮', '⁺', '▨', '╨', 'ⓥ', '♈', '❃', '☝', '✻', '⊇', '≻', '♘', '♞', \n",
    "        '◂', '✟', '⌠', '✠', '☚', '✥', '❊', 'ⓒ', '⌈', '❅', 'ⓡ', '♧', 'ⓞ', '▭', '❱', 'ⓣ', '∟', '☕', '♺', '∵', '⍝', 'ⓑ', \n",
    "        '✵', '✣', '٭', '♆', 'ⓘ', '∶', '⚜', '◞', '்', '✹', '➥', '↕', '̳', '∷', '✋', '➧', '∋', '̿', 'ͧ', '┅', '⥤', '⬆', '⋱', \n",
    "        '☄', '↖', '⋮', '۔', '♌', 'ⓛ', '╕', '♓', '❯', '♍', '▋', '✺', '⭐', '✾', '♊', '➣', '▿', 'ⓑ', '♉', '⏠', '◾', '▹', \n",
    "        '⩽', '↦', '╥', '⍵', '⌋', '։', '➨', '∮', '⇥', 'ⓗ', 'ⓓ', '⁻', '⎝', '⌥', '⌉', '◔', '◑', '✼', '♎', '♐', '╪', '⊚', \n",
    "        '☒', '⇤', 'ⓜ', '⎠', '◐', '⚠', '╞', '◗', '⎕', 'ⓨ', '☟', 'ⓟ', '♟', '❈', '↬', 'ⓓ', '◻', '♮', '❙', '♤', '∉', '؛', \n",
    "        '⁂', 'ⓝ', '־', '♑', '╫', '╓', '╳', '⬅', '☔', '☸', '┄', '╧', '׃', '⎢', '❆', '⋄', '⚫', '̏', '☏', '➞', '͂', '␙', \n",
    "        'ⓤ', '◟', '̊', '⚐', '✙', '↙', '̾', '℘', '✷', '⍺', '❌', '⊢', '▵', '✅', 'ⓖ', '☨', '▰', '╡', 'ⓜ', '☤', '∽', '╘', \n",
    "        '˹', '↨', '♙', '⬇', '♱', '⌡', '⠀', '╛', '❕', '┉', 'ⓟ', '̀', '♖', 'ⓚ', '┆', '⎜', '◜', '⚾', '⤴', '✇', '╟', '⎛', \n",
    "        '☩', '➲', '➟', 'ⓥ', 'ⓗ', '⏝', '◃', '╢', '↯', '✆', '˃', '⍴', '❇', '⚽', '╒', '̸', '♜', '☓', '➳', '⇄', '☬', '⚑', \n",
    "        '✐', '⌃', '◅', '▢', '❐', '∊', '☈', '॥', '⎮', '▩', 'ு', '⊹', '‵', '␔', '☊', '➸', '̌', '☿', '⇉', '⊳', '╙', 'ⓦ', \n",
    "        '⇣', '｛', '̄', '↝', '⎟', '▍', '❗', '״', '΄', '▞', '◁', '⛄', '⇝', '⎪', '♁', '⇠', '☇', '✊', 'ி', '｝', '⭕', '➘', \n",
    "        '⁀', '☙', '❛', '❓', '⟲', '⇀', '≲', 'ⓕ', '⎥', '\\u06dd', 'ͤ', '₋', '̱', '̎', '♝', '≳', '▙', '➭', '܀', 'ⓖ', '⇛', '▊', \n",
    "        '⇗', '̷', '⇱', '℅', 'ⓧ', '⚛', '̐', '̕', '⇌', '␀', '≌', 'ⓦ', '⊤', '̓', '☦', 'ⓕ', '▜', '➙', 'ⓨ', '⌨', '◮', '☷', \n",
    "        '◍', 'ⓚ', '≔', '⏩', '⍳', '℞', '┋', '˻', '▚', '≺', 'ْ', '▟', '➻', '̪', '⏪', '̉', '⎞', '┇', '⍟', '⇪', '▎', '⇦', '␝', \n",
    "        '⤷', '≖', '⟶', '♗', '̴', '♄', 'ͨ', '̈', '❜', '̡', '▛', '✁', '➩', 'ா', '˂', '↥', '⏎', '⎷', '̲', '➖', '↲', '⩵', '̗', '❢', \n",
    "        '≎', '⚔', '⇇', '̑', '⊿', '̖', '☍', '➹', '⥊', '⁁', '✢']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#替换特殊的符号\n",
    "def clean_punct(x):\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "train[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_punct(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 200000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 72 # max number of words in a question to use #99.99%\n",
    "\n",
    "## fill up the missing values\n",
    "X = train[\"question_text\"].fillna(\"_####_\").values\n",
    "X_test = test[\"question_text\"].fillna(\"_####_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "tokenizer.fit_on_texts(list(X)+list(X_test))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "## Pad the sentences \n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "Y = train['target'].values\n",
    "\n",
    "sub = test[['qid']]\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223748, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入词向量，并作一些简单处理使得单词更加全面\n",
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index)+1\n",
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) \n",
    "                            for o in open(EMBEDDING_FILE) \n",
    "                            if o.split(\" \")[0] in word_index or o.split(\" \")[0].lower() in word_index)\n",
    "\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:                            #原单词\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        elif embeddings_index.get(word.capitalize()) is not None:    #首字母大写，其他小写\n",
    "            embedding_matrix[i] = embeddings_index.get(word.capitalize())\n",
    "        elif embeddings_index.get(word.upper()) is not None:         #小写单词\n",
    "            embedding_matrix[i] = embeddings_index.get(word.upper())\n",
    "    del embeddings_index\n",
    "    gc.collect()        \n",
    "    return embedding_matrix \n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) \n",
    "                            for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') \n",
    "                            if len(o)>100 and (o.split(\" \")[0] in word_index or o.split(\" \")[0].lower() in word_index))\n",
    "\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        elif embeddings_index.get(word.capitalize()) is not None:\n",
    "            embedding_matrix[i] = embeddings_index.get(word.capitalize())\n",
    "        elif embeddings_index.get(word.upper()) is not None:\n",
    "            embedding_matrix[i] = embeddings_index.get(word.upper())\n",
    "        \n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return embedding_matrix\n",
    "\n",
    "seed_everything()\n",
    "embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix_3 = load_para(word_index)\n",
    "\n",
    "#词向量按不同权重平均\n",
    "embedding_matrix = np.mean((1.28*embedding_matrix_1, 0.72*embedding_matrix_3), axis=0)\n",
    "del embedding_matrix_1, embedding_matrix_3\n",
    "gc.collect()\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义attention层和adamW\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.RandomUniform(seed=10000)\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay') # decoupled weight decay (2/4)\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p # decoupled weight decay (4/4)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型结构1\n",
    "def LSTM_GRU(spatialdropout=0.20, rnn_units=64, weight_decay=0.07):\n",
    "    K.clear_session()       \n",
    "    x_input = Input(shape=(maxlen,))\n",
    "    \n",
    "    emb = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False, name='Embedding')(x_input)\n",
    "    emb = SpatialDropout1D(spatialdropout, seed=1024)(emb)\n",
    "\n",
    "    rnn1 = Bidirectional(CuDNNLSTM(rnn_units, return_sequences=True, kernel_initializer=glorot_uniform(seed=111100), \n",
    "                           recurrent_initializer=Orthogonal(gain=1.0, seed=123000)))(emb)\n",
    "    rnn2 = Bidirectional(CuDNNGRU(rnn_units, return_sequences=True, kernel_initializer=glorot_uniform(seed=111000), \n",
    "                           recurrent_initializer=Orthogonal(gain=1.0, seed=1203000)))(rnn1)\n",
    "\n",
    "    x = concatenate([rnn1, rnn2])\n",
    "    x = GlobalMaxPooling1D()(x)  \n",
    "    x_output = Dense(1, activation='sigmoid', kernel_initializer=glorot_uniform(seed=111100))(x)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=AdamW(weight_decay=weight_decay),)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型结构2\n",
    "#epoch=7\n",
    "def poolRNN(spatialdropout=0.2, gru_units=128, weight_decay=0.04):\n",
    "    K.clear_session()\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(max_features,\n",
    "                                embed_size,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False)(inp)\n",
    "    embedding_layer = SpatialDropout1D(spatialdropout, seed=1024)(embedding_layer)\n",
    "\n",
    "    rnn_1 = Bidirectional(CuDNNGRU(gru_units, return_sequences=True, \n",
    "                                   kernel_initializer=glorot_uniform(seed=10000), \n",
    "                                   recurrent_initializer=Orthogonal(gain=1.0, seed=123000)))(embedding_layer)\n",
    "\n",
    "    last = Lambda(lambda t: t[:, -1], name='last')(rnn_1)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_1)\n",
    "    attn = AttentionWeightedAverage()(rnn_1)\n",
    "    average = GlobalAveragePooling1D()(rnn_1)\n",
    "\n",
    "    c = concatenate([last, maxpool, attn], axis=1)\n",
    "    c = Reshape((3, -1))(c)\n",
    "    c = Lambda(lambda x:K.sum(x, axis=1))(c)\n",
    "    x = BatchNormalization()(c)\n",
    "    x = Dense(200, activation='relu', kernel_initializer=glorot_uniform(seed=111000))(x)\n",
    "    x = Dropout(0.2, seed=1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\", kernel_initializer=glorot_uniform(seed=111000))(x)\n",
    "    model = Model(inputs=inp, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=AdamW(weight_decay=weight_decay))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型结构3\n",
    "def BiLSTM_CNN(spatialdropout=0.2, rnn_units=128, filters=[100, 80, 30, 12], weight_decay=0.10):\n",
    "    K.clear_session()       \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(rate=spatialdropout, seed=10000)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(rnn_units, return_sequences=True, \n",
    "                               kernel_initializer=glorot_uniform(seed=111000), \n",
    "                               recurrent_initializer=Orthogonal(gain=1.0, seed=123000)))(x)\n",
    "\n",
    "    x1 = Conv1D(filters=filters[0], activation='relu', kernel_size=1, \n",
    "                padding='same', kernel_initializer=glorot_uniform(seed=110000))(x)\n",
    "    x2 = Conv1D(filters=filters[1], activation='relu', kernel_size=2, \n",
    "                padding='same', kernel_initializer=glorot_uniform(seed=120000))(x)\n",
    "    x3 = Conv1D(filters=filters[2], activation='relu', kernel_size=3, \n",
    "                padding='same', kernel_initializer=glorot_uniform(seed=130000))(x)\n",
    "    x4 = Conv1D(filters=filters[3], activation='relu', kernel_size=5, \n",
    "                padding='same', kernel_initializer=glorot_uniform(seed=140000))(x)\n",
    "\n",
    "    \n",
    "    x1 = GlobalMaxPool1D()(x1)\n",
    "    x2 = GlobalMaxPool1D()(x2)\n",
    "    x3 = GlobalMaxPool1D()(x3)\n",
    "    x4 = GlobalMaxPool1D()(x4)\n",
    "\n",
    "    c = concatenate([x1, x2, x3, x4])\n",
    "    x = Dense(200, activation='relu', kernel_initializer=glorot_uniform(seed=111000))(c)\n",
    "    x = Dropout(0.2, seed=10000)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\", kernel_initializer=glorot_uniform(seed=110000))(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=AdamW(weight_decay=weight_decay))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型结构4\n",
    "#epoch=7\n",
    "def singleRNN(spatialdropout=0.20, rnn_units=120, weight_decay=0.08):\n",
    "    K.clear_session()\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(max_features,\n",
    "                                embed_size,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False)(inp)\n",
    "    embedding_layer = SpatialDropout1D(spatialdropout, seed=1024)(embedding_layer)\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(rnn_units, return_sequences=True, \n",
    "                                   kernel_initializer=glorot_uniform(seed=111000), \n",
    "                                   recurrent_initializer=Orthogonal(gain=1.0, seed=123000)))(embedding_layer)\n",
    "\n",
    "    x = AttentionWeightedAverage()(x)\n",
    "    x = Dense(100, kernel_initializer=glorot_uniform(seed=111000))(x)\n",
    "    x = Dropout(0.12, seed=1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\", kernel_initializer=glorot_uniform(seed=111000))(x)\n",
    "    model = Model(inputs=inp, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=AdamW(weight_decay=weight_decay))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义f1函数，可以得出阀值\n",
    "def f1_smart(y_true, y_pred):\n",
    "    args = np.argsort(y_pred)\n",
    "    tp = y_true.sum()\n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD1\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "LSTM_GRU(spatialdropout=0.20, rnn_units=64, weight_decay=0.07)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "train logloss:[0.11759099725726352, 0.10276792913609609, 0.09815277054641339, 0.09491142912159056, 0.09241925636582358, 0.09025414252364734, 0.08845839995601772, 0.08209750478683663]\n",
      "val logloss:[0.10240384631193913, 0.09824021655529716, 0.09553803843447324, 0.0951386381953506, 0.09458865939379313, 0.09407106672741923, 0.09436700971764865, 0.0930861053193227]\n",
      "Optimal F1: 0.6972 at threshold: 0.3174\n",
      "\n",
      "FOLD2\n",
      "poolRNN(spatialdropout=0.2, gru_units=128, weight_decay=0.04)\n",
      "train logloss:[0.1378395096501395, 0.10408810704023005, 0.09918862019047445, 0.09501516070182062, 0.09200669502816546, 0.08905362274665601, 0.08123060789955651, 0.07857429243100242]\n",
      "val logloss:[0.10481595637463342, 0.10003071809166068, 0.09845848436463307, 0.09750677328991017, 0.09600304322512601, 0.09609046689844558, 0.09558034873207101, 0.09671610909516076]\n",
      "Optimal F1: 0.6968 at threshold: 0.3187\n",
      "\n",
      "FOLD3\n",
      "BiLSTM_CNN(spatialdropout=0.2, rnn_units=128, filters=[100, 90, 30, 12], weight_decay=0.10)\n",
      "train logloss:[0.12900612620327062, 0.10376516927013535, 0.09878360992509869, 0.09038259310992533, 0.08694716498288992, 0.0848109503364306, 0.08036620421220377]\n",
      "val logloss:[0.10302676431732995, 0.09802678955923529, 0.10584209382977096, 0.0929166567946175, 0.09249943379988756, 0.09331686495998302, 0.09322540601246512]\n",
      "Optimal F1: 0.7046 at threshold: 0.3471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#定义训练。\n",
    "kfold = StratifiedKFold(n_splits=7, random_state=10, shuffle=True)\n",
    "bestscore = []\n",
    "bestloss = []\n",
    "y_test = np.zeros((X_test.shape[0], ))\n",
    "oof = np.zeros((X.shape[0], ))\n",
    "epochs = [8, 8, 7, 6]\n",
    "val_list = []\n",
    "for i, (train_index, valid_index) in enumerate(kfold.split(X, Y)):\n",
    "    val_list += list(valid_index)\n",
    "    print('FOLD%s'%(i+1))\n",
    "    X_train, X_val, Y_train, Y_val = X[train_index], X[valid_index], Y[train_index], Y[valid_index]\n",
    "    filepath=\"weights_best.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0, verbose=0)\n",
    "    callbacks = [checkpoint, reduce_lr]\n",
    "    if i == 0:\n",
    "        model = LSTM_GRU(spatialdropout=0.20, rnn_units=64, weight_decay=0.07)\n",
    "        print('LSTM_GRU(spatialdropout=0.20, rnn_units=64, weight_decay=0.07)')\n",
    "    elif i == 1:\n",
    "        model = poolRNN(spatialdropout=0.2, gru_units=128, weight_decay=0.04)\n",
    "        print('poolRNN(spatialdropout=0.2, gru_units=128, weight_decay=0.04)')\n",
    "    elif i == 2:\n",
    "        model = BiLSTM_CNN(spatialdropout=0.2, rnn_units=128, filters=[100, 90, 30, 12], weight_decay=0.10)\n",
    "        print('BiLSTM_CNN(spatialdropout=0.2, rnn_units=128, filters=[100, 90, 30, 12], weight_decay=0.10)')\n",
    "    model.fit(X_train, Y_train, batch_size=512, epochs=epochs[i], \n",
    "              validation_data=(X_val, Y_val), verbose=0, callbacks=callbacks, \n",
    "              #class_weight={0:1, 1:1.25}\n",
    "             )\n",
    "    print(\"train logloss:%s\"%model.history.history['loss'])\n",
    "    print(\"val logloss:%s\"%model.history.history['val_loss'])\n",
    "    y_pred = model.predict([X_val], batch_size=1024, verbose=2)\n",
    "    y_test += np.squeeze(model.predict([X_test], batch_size=1024, verbose=2))/3\n",
    "    oof[valid_index] = np.squeeze(y_pred)\n",
    "    f1, threshold = f1_smart(np.squeeze(Y_val), np.squeeze(y_pred))\n",
    "    print('Optimal F1: {:.4f} at threshold: {:.4f}\\n'.format(f1, threshold))\n",
    "    bestscore.append(threshold)\n",
    "    if i == 2:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal F1: 0.6991 at threshold: 0.3325\n"
     ]
    }
   ],
   "source": [
    "f1, threshold = f1_smart(np.squeeze(Y[val_list]), np.squeeze(oof[val_list]))\n",
    "print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape((-1, 1)) \n",
    "pred_test_y = (y_test>threshold).astype(int) \n",
    "sub['prediction'] = pred_test_y \n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
